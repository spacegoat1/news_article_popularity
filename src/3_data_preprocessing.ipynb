{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf29eec-8cb5-46d7-8e00-69533756f499",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "- Discuss how you split the dataset and why.\n",
    "- Is your dataset IID?\n",
    "- Does it have group structure?\n",
    "- Is it a time-series data?\n",
    "- How should you split the dataset given your ML question to best mimic future use when you deploy the model?\n",
    "- Apply MinMaxEncoder or StandardScaler on the continuous features\n",
    "- Apply OneHotEncoder or OrdinalEncoder on categorical features\n",
    "- Describe why you chose a particular preprocessor for each feature.\n",
    "- How many features do you have in the preprocessed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ab87e-3a6e-4fe4-984f-b34ab490ec0d",
   "metadata": {},
   "source": [
    "1. The dataset\n",
    "- Discuss how you split the dataset and why.\n",
    "- Is your dataset IID?\n",
    "- Does it have group structure?\n",
    "- Is it a time-series data?\n",
    "\n",
    "The dataset consists of records of online articles published by Mashable. Each row corresponds to a single article. There are a total of 39,644 articles. \n",
    "\n",
    "The target variable is 'popular' - this is a binary variable where 0 corresponds to not-popular and 1 corresponds to popular. This variable is derived from the 'shares' variable present in the original dataset. Shares is the number of shares for a given article, and we set a threshold = 1400, above which an article is considered popular. \n",
    "\n",
    "The features list various characteristics about the articles, including some NLP based metrics, the number of tokens in the title, in the content, the number of images and videos, the topic and the day of the week on which the article was published. There are 58 features. \n",
    "\n",
    "Since there is no group structure or time-series structure to the data, the data can be considered IID. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203f23f-ca89-4d68-8cc6-a2ef2d89181c",
   "metadata": {},
   "source": [
    "2. How should you split the dataset given your ML question to best mimic future use when you deploy the model?\n",
    "\n",
    "The point of this project was to be able to predict how popular an article would be *before publication*, and make recommendations to increase popularity. \n",
    "\n",
    "Each of the feature variables is available to us prior to publication in a production environment, so we can safely split the dataset into the standard train:validation:test split. Here, I use a split of 70:20:10, which gives us 27751, 7929, 3964 articles for the train, validation and test sets respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e4b19-0e2f-4e24-a4fd-c42c2e3ae3fb",
   "metadata": {},
   "source": [
    "3. Feature Encoding\n",
    "- Apply MinMaxEncoder or StandardScaler on the continuous features\n",
    "- Apply OneHotEncoder or OrdinalEncoder on categorical features\n",
    "- Describe why you chose a particular preprocessor for each feature.\n",
    "- How many features do you have in the preprocessed data?\n",
    "\n",
    "On the continuous features, I am applying StandardScaler for all features except the following:\n",
    "\n",
    "The reason for applying StandardScaler is that the histograms for these features are very fat tailed, and a MinMaxEncoder would clump most values to the leftmost part of the histogram. \n",
    "\n",
    "For the categorical features, I use OneHotEncoder. Eg. the data_channel feature tells us what the topic of the article is, and it is not possibly to order topics. Similarly, I use OneHot encoding on the is_weekend feature and the day_of_week feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb229770-d3b4-479b-a50a-0410308d84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_paths import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdfcfb3-6b76-4134-b989-2c80a64768da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39644, 51)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>topic</th>\n",
       "      <th>popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...      731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...      731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...      731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...      731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/      731.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0            12.0             219.0         0.663594               1.0   \n",
       "1             9.0             255.0         0.604743               1.0   \n",
       "2             9.0             211.0         0.575130               1.0   \n",
       "3             9.0             531.0         0.503788               1.0   \n",
       "4            13.0            1072.0         0.415646               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                  0.815385        4.0             2.0       1.0  ...   \n",
       "1                  0.791946        3.0             1.0       1.0  ...   \n",
       "2                  0.663866        3.0             1.0       1.0  ...   \n",
       "3                  0.665635        9.0             0.0       1.0  ...   \n",
       "4                  0.540890       19.0            19.0      20.0  ...   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                 -0.600              -0.200000            0.500000   \n",
       "1                 -0.125              -0.100000            0.000000   \n",
       "2                 -0.800              -0.133333            0.000000   \n",
       "3                 -0.600              -0.166667            0.000000   \n",
       "4                 -0.500              -0.050000            0.454545   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                 -0.187500                0.000000   \n",
       "1                  0.000000                0.500000   \n",
       "2                  0.000000                0.500000   \n",
       "3                  0.000000                0.500000   \n",
       "4                  0.136364                0.045455   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  day_of_week          topic  popular  \n",
       "0                      0.187500     593       Monday  Entertainment        0  \n",
       "1                      0.000000     711       Monday       Business        0  \n",
       "2                      0.000000    1500       Monday       Business        1  \n",
       "3                      0.000000    1200       Monday  Entertainment        0  \n",
       "4                      0.136364     505       Monday           Tech        0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_csv_for_preprocessing)\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c35a888-2559-44f7-9eb4-6638c3d86116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31715, 48)\n",
      "(3964, 48)\n",
      "(3965, 48)\n",
      "(31715,)\n",
      "(3964,)\n",
      "(3965,)\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 7\n",
    "\n",
    "USING_BASIC_SPLIT = True\n",
    "\n",
    "if USING_BASIC_SPLIT:\n",
    "    non_predictive_columns = ['url', 'timedelta']\n",
    "    target_column = 'popular'\n",
    "\n",
    "    y = data[target_column]\n",
    "    X = data[[x for x in data.columns if x not in non_predictive_columns and x != target_column]]\n",
    "\n",
    "    # Splitting the data - BASIC TRAIN TEST SPLIT\n",
    "\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X, y, train_size=0.8, random_state=RANDOM_STATE)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, train_size=0.5, random_state=RANDOM_STATE)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_val.shape)\n",
    "    print(X_test.shape)\n",
    "\n",
    "    print(y_train.shape)\n",
    "    print(y_val.shape)\n",
    "    print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b57465d-9005-4de6-9aec-39cf15184bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_BASIC_SPLIT:\n",
    "    # Identifying quantiles (mulitples of 10)\n",
    "    groups_percentiles = data['shares'].quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]).to_dict()\n",
    "\n",
    "    def get_group_given_shares(groups_percentiles, i):\n",
    "        for kx, k in enumerate(sorted(groups_percentiles.keys())):\n",
    "            if i > groups_percentiles[k]:\n",
    "                continue\n",
    "            return k\n",
    "            if i > groups_percentiles[ sorted(groups_percentiles.keys())[kx-1] ]:\n",
    "                return sorted(groups_percentiles.keys())[kx]\n",
    "        return 1.0\n",
    "\n",
    "    groups = []\n",
    "    for i in data['shares']:\n",
    "        groups.append(get_group_given_shares(groups_percentiles, i))\n",
    "\n",
    "    non_predictive_columns = ['url', 'timedelta']\n",
    "    target_column = 'popular'\n",
    "\n",
    "    data['groups'] = groups\n",
    "    y = data[target_column]\n",
    "    X = data[[x for x in data.columns if x not in non_predictive_columns and x != target_column]]\n",
    "    \n",
    "    # Using stratification to make sure that articles of all levels of popularity are covered in all sets\n",
    "\n",
    "    # First a stratified split into train and other\n",
    "    groups = X['groups']\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X, y, train_size = 0.8, stratify=groups, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Then a stratified split into val and test\n",
    "    groups_other = X_other[\"groups\"]\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, train_size = 0.5, stratify=groups_other, random_state=RANDOM_STATE)\n",
    "    \n",
    "    # Dropping the groups column since it was only used for stratitfication\n",
    "    X_train = X_train.drop('groups', axis=1)\n",
    "    X_val = X_val.drop('groups', axis=1)\n",
    "    X_test = X_test.drop('groups', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cc2850-08e2-41cb-89b7-66ab333ad42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31715, 48)\n",
      "(31715,)\n",
      "(3964, 48)\n",
      "(3964,)\n",
      "(3965, 48)\n",
      "(3965,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a85ff0-f309-4d73-8554-ff6d6c1800a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Feature Types for encoding\n",
    "\n",
    "onehot_ftrs = ['topic', 'is_weekend', 'day_of_week']\n",
    "# ordinal_ftrs = ['day_of_week']\n",
    "# ordinal_cats = [['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']]\n",
    "minmax_ftrs = ['n_tokens_title',\n",
    "                'average_token_length',\n",
    "                'num_keywords',\n",
    "                'LDA_00',\n",
    "                'LDA_01',\n",
    "                'LDA_02',\n",
    "                'LDA_03',\n",
    "                'LDA_04',\n",
    "                'title_subjectivity',\n",
    "                'title_sentiment_polarity',\n",
    "                'abs_title_subjectivity',\n",
    "                'abs_title_sentiment_polarity']\n",
    "standard_ftrs = ['n_tokens_content',\n",
    "                'n_unique_tokens',\n",
    "                'n_non_stop_words',\n",
    "                'n_non_stop_unique_tokens',\n",
    "                'num_hrefs',\n",
    "                'num_self_hrefs',\n",
    "                'num_imgs',\n",
    "                'num_videos',\n",
    "                'kw_min_min',\n",
    "                'kw_max_min',\n",
    "                'kw_avg_min',\n",
    "                'kw_min_max',\n",
    "                'kw_max_max',\n",
    "                'kw_avg_max',\n",
    "                'kw_min_avg',\n",
    "                'kw_max_avg',\n",
    "                'kw_avg_avg',\n",
    "                'self_reference_min_shares',\n",
    "                'self_reference_max_shares',\n",
    "                'self_reference_avg_sharess',\n",
    "                'global_subjectivity',\n",
    "                'global_sentiment_polarity',\n",
    "                'global_rate_positive_words',\n",
    "                'global_rate_negative_words',\n",
    "                'rate_positive_words',\n",
    "                'rate_negative_words',\n",
    "                'avg_positive_polarity',\n",
    "                'min_positive_polarity',\n",
    "                'max_positive_polarity',\n",
    "                'avg_negative_polarity',\n",
    "                'min_negative_polarity',\n",
    "                'max_negative_polarity',]\n",
    "\n",
    "# print([x for x in data.columns if x not in onehot_ftrs+ordinal_ftrs+minmax_ftrs+standard_ftrs])\n",
    "save_list_to_pkl(onehot_ftrs, 'onehot_ftrs.pkl')\n",
    "save_list_to_pkl(minmax_ftrs, 'minmax_ftrs.pkl')\n",
    "save_list_to_pkl(standard_ftrs, 'standard_ftrs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c0079c-0357-4e5b-b678-eac2dc11decd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31715, 60)\n",
      "(3964, 60)\n",
      "(3965, 60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ,  0.        , ...,  0.79044934,\n",
       "         0.79389477,  0.37938288],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.14082918,\n",
       "        -1.64618344,  0.60437983],\n",
       "       [ 1.        ,  0.        ,  0.        , ...,  0.86351945,\n",
       "         0.76518797,  0.37938288],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -1.55291375,\n",
       "        -0.95722018,  0.07938695],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.66246029,\n",
       "         0.76518797,  0.07938695],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.43708197,\n",
       "         0.07622471,  0.07938695]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'), onehot_ftrs), \n",
    "                 ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "                 ('standard', StandardScaler(), standard_ftrs),])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train)\n",
    "X_val_prep = clf.transform(X_val)\n",
    "X_test_prep = clf.transform(X_test)\n",
    "\n",
    "print(X_train_prep.shape)\n",
    "print(X_val_prep.shape)\n",
    "print(X_test_prep.shape)\n",
    "X_train_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c04c9a8-9203-4699-a472-3fa61c4b6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preprocessed_data(X_train_prep, preprocessed_data_path_train)\n",
    "save_preprocessed_data(X_val_prep, preprocessed_data_path_val)\n",
    "save_preprocessed_data(X_test_prep, preprocessed_data_path_test)\n",
    "\n",
    "save_preprocessed_data(y_train, preprocessed_data_y_train)\n",
    "save_preprocessed_data(y_val, preprocessed_data_y_val)\n",
    "save_preprocessed_data(y_test, preprocessed_data_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a96e4c-a5f6-4bd3-b935-b39a233333f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [t for t in clf['preprocessor'].transformers_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0873bc5-8e96-434b-8faf-4ddbd5d22d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_features = feature_names[0][1].get_feature_names()\n",
    "all_features_pp = list(oe_features) + minmax_ftrs + standard_ftrs\n",
    "save_list_to_pkl(all_features_pp, 'processed_features_list.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
