{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf29eec-8cb5-46d7-8e00-69533756f499",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "- Discuss how you split the dataset and why.\n",
    "- Is your dataset IID?\n",
    "- Does it have group structure?\n",
    "- Is it a time-series data?\n",
    "- How should you split the dataset given your ML question to best mimic future use when you deploy the model?\n",
    "- Apply MinMaxEncoder or StandardScaler on the continuous features\n",
    "- Apply OneHotEncoder or OrdinalEncoder on categorical features\n",
    "- Describe why you chose a particular preprocessor for each feature.\n",
    "- How many features do you have in the preprocessed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ab87e-3a6e-4fe4-984f-b34ab490ec0d",
   "metadata": {},
   "source": [
    "1. The dataset\n",
    "- Discuss how you split the dataset and why.\n",
    "- Is your dataset IID?\n",
    "- Does it have group structure?\n",
    "- Is it a time-series data?\n",
    "\n",
    "The dataset consists of records of online articles published by Mashable. Each row corresponds to a single article. There are a total of 39,644 articles. \n",
    "\n",
    "The target variable is 'popular' - this is a binary variable where 0 corresponds to not-popular and 1 corresponds to popular. This variable is derived from the 'shares' variable present in the original dataset. Shares is the number of shares for a given article, and we set a threshold = 1400, above which an article is considered popular. \n",
    "\n",
    "The features list various characteristics about the articles, including some NLP based metrics, the number of tokens in the title, in the content, the number of images and videos, the topic and the day of the week on which the article was published. There are 58 features. \n",
    "\n",
    "Since there is no group structure or time-series structure to the data, the data can be considered IID. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203f23f-ca89-4d68-8cc6-a2ef2d89181c",
   "metadata": {},
   "source": [
    "2. How should you split the dataset given your ML question to best mimic future use when you deploy the model?\n",
    "\n",
    "The point of this project was to be able to predict how popular an article would be *before publication*, and make recommendations to increase popularity. \n",
    "\n",
    "Each of the feature variables is available to us prior to publication in a production environment, so we can safely split the dataset into the standard train:validation:test split. Here, I use a split of 70:20:10, which gives us 27751, 7929, 3964 articles for the train, validation and test sets respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e4b19-0e2f-4e24-a4fd-c42c2e3ae3fb",
   "metadata": {},
   "source": [
    "3. Feature Encoding\n",
    "- Apply MinMaxEncoder or StandardScaler on the continuous features\n",
    "- Apply OneHotEncoder or OrdinalEncoder on categorical features\n",
    "- Describe why you chose a particular preprocessor for each feature.\n",
    "- How many features do you have in the preprocessed data?\n",
    "\n",
    "On the continuous features, I am applying StandardScaler for all features except the following:\n",
    "\n",
    "The reason for applying StandardScaler is that the histograms for these features are very fat tailed, and a MinMaxEncoder would clump most values to the leftmost part of the histogram. \n",
    "\n",
    "From the categorical features, I use OneHotEncoder on data_channel, since that tells us what the topic of the article is, and it is not possibly to order topics. Similarly, I use OneHot encoding on the is_weekend feature. \n",
    "\n",
    "The last remaining categorical feature is the day_of_week feature, for which I use the OrdinalEncoder since the days of the week occur in a sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb229770-d3b4-479b-a50a-0410308d84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_paths import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fdfcfb3-6b76-4134-b989-2c80a64768da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39644, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>data_channel</th>\n",
       "      <th>popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Business</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...      731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...      731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...      731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...      731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/      731.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0            12.0             219.0         0.663594               1.0   \n",
       "1             9.0             255.0         0.604743               1.0   \n",
       "2             9.0             211.0         0.575130               1.0   \n",
       "3             9.0             531.0         0.503788               1.0   \n",
       "4            13.0            1072.0         0.415646               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                  0.815385        4.0             2.0       1.0  ...   \n",
       "1                  0.791946        3.0             1.0       1.0  ...   \n",
       "2                  0.663866        3.0             1.0       1.0  ...   \n",
       "3                  0.665635        9.0             0.0       1.0  ...   \n",
       "4                  0.540890       19.0            19.0      20.0  ...   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                 -0.600              -0.200000   \n",
       "1              -0.118750                 -0.125              -0.100000   \n",
       "2              -0.466667                 -0.800              -0.133333   \n",
       "3              -0.369697                 -0.600              -0.166667   \n",
       "4              -0.220192                 -0.500              -0.050000   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                 -0.187500                0.000000   \n",
       "1            0.000000                  0.000000                0.500000   \n",
       "2            0.000000                  0.000000                0.500000   \n",
       "3            0.000000                  0.000000                0.500000   \n",
       "4            0.454545                  0.136364                0.045455   \n",
       "\n",
       "   abs_title_sentiment_polarity  day_of_week   data_channel  popular  \n",
       "0                      0.187500       Monday  Entertainment        0  \n",
       "1                      0.000000       Monday       Business        0  \n",
       "2                      0.000000       Monday       Business        1  \n",
       "3                      0.000000       Monday  Entertainment        0  \n",
       "4                      0.136364       Monday           Tech        0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_csv_for_preprocessing)\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c35a888-2559-44f7-9eb4-6638c3d86116",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 7\n",
    "\n",
    "non_predictive_columns = ['url', 'timedelta']\n",
    "target_column = 'popular'\n",
    "\n",
    "y = data[target_column]\n",
    "X = data[[x for x in data.columns if x not in non_predictive_columns and x != target_column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8ca8683-ff1e-4176-a5de-f30deba5cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27750, 47)\n",
      "(7929, 47)\n",
      "(3965, 47)\n",
      "(27750,)\n",
      "(7929,)\n",
      "(3965,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(X, y, train_size=0.7, random_state=RANDOM_STATE)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, train_size=0.66667, random_state=RANDOM_STATE)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61a85ff0-f309-4d73-8554-ff6d6c1800a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Feature Types for encoding\n",
    "\n",
    "onehot_ftrs = ['data_channel', 'is_weekend']\n",
    "ordinal_ftrs = ['day_of_week']\n",
    "ordinal_cats = [['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']]\n",
    "minmax_ftrs = ['n_tokens_title',\n",
    "                'average_token_length',\n",
    "                'num_keywords',\n",
    "                'LDA_00',\n",
    "                'LDA_01',\n",
    "                'LDA_02',\n",
    "                'LDA_03',\n",
    "                'LDA_04',\n",
    "                'title_subjectivity',\n",
    "                'title_sentiment_polarity',\n",
    "                'abs_title_subjectivity',\n",
    "                'abs_title_sentiment_polarity']\n",
    "standard_ftrs = ['n_tokens_content',\n",
    "                'n_unique_tokens',\n",
    "                'n_non_stop_words',\n",
    "                'n_non_stop_unique_tokens',\n",
    "                'num_hrefs',\n",
    "                'num_self_hrefs',\n",
    "                'num_imgs',\n",
    "                'num_videos',\n",
    "                'kw_min_min',\n",
    "                'kw_max_min',\n",
    "                'kw_avg_min',\n",
    "                'kw_min_max',\n",
    "                'kw_max_max',\n",
    "                'kw_avg_max',\n",
    "                'kw_min_avg',\n",
    "                'kw_max_avg',\n",
    "                'kw_avg_avg',\n",
    "                'self_reference_min_shares',\n",
    "                'self_reference_max_shares',\n",
    "                'self_reference_avg_sharess',\n",
    "                'global_subjectivity',\n",
    "                'global_sentiment_polarity',\n",
    "                'global_rate_positive_words',\n",
    "                'global_rate_negative_words',\n",
    "                'rate_positive_words',\n",
    "                'rate_negative_words',\n",
    "                'avg_positive_polarity',\n",
    "                'min_positive_polarity',\n",
    "                'max_positive_polarity',\n",
    "                'avg_negative_polarity',\n",
    "                'min_negative_polarity',\n",
    "                'max_negative_polarity',]\n",
    "\n",
    "# print([x for x in data.columns if x not in onehot_ftrs+ordinal_ftrs+minmax_ftrs+standard_ftrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54c0079c-0357-4e5b-b678-eac2dc11decd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27750, 54)\n",
      "(7929, 54)\n",
      "(3965, 54)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  1.0935568 ,\n",
       "         0.7643038 ,  0.60840291],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.02324474,\n",
       "        -0.95633221,  0.60840291],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.98890021,\n",
       "        -1.30045941,  0.07790326],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -1.55803048,\n",
       "        -0.95633221,  0.07790326],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.66251059,\n",
       "         0.7643038 ,  0.07790326],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.43959619,\n",
       "         0.07604939,  0.07790326]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'), onehot_ftrs), \n",
    "                 ('ordinal', OrdinalEncoder(categories=ordinal_cats), ordinal_ftrs), \n",
    "                 ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "                 ('standard', StandardScaler(), standard_ftrs),])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train)\n",
    "X_val_prep = clf.transform(X_val)\n",
    "X_test_prep = clf.transform(X_test)\n",
    "\n",
    "print(X_train_prep.shape)\n",
    "print(X_val_prep.shape)\n",
    "print(X_test_prep.shape)\n",
    "X_train_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04c9a8-9203-4699-a472-3fa61c4b6a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
